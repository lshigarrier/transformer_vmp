name : pems_lstm1
model : weights.pt
# load existing weights from ./models/[name]/[model]
load : False
# Training or inference
train : True
seed : 42
gpu_number : ~
# Use wandb
wandb : True

# model

# use transformer or lstm
transfo : False
# nb of multi-head (n), nb of heads per multi-head (h)
dim : ~
# use embedding layers
embed : False
# embedding dimensions, if embed: we must have emb[-1]%h = 0, else: we must have input_dim%h=0
emb : [400,]
# classifiers dimensions, we must have emb[-1] = clas[0]
clas : ~
# use uncertainty quantification
vmp : False
# residual connection mode for variance computation : identity, independence, taylor
residual : ~

# training

epochs : 50
batch_size : 32
optimizer : adam
learning_rate : 1.e-3
# End factor to multiply the learning rate at the last epoch
end_factor : 1.e-1
l2_reg : 0.
# l2 regularization hyperparameter in ELBO
kl_factor : ~
# gradient clipping
clip : 10
# nb of workers for dataloaders
workers : 4
# tolerance (0 < tol < 1) for:
# - removing nan -> tol
# - removing posinf -> 1/tol, and neginf -> -1/tol
# - clamping positive numbers between tol and 1/tol
tol : 1.e-6
# initialization factor for variance
var_init : ~

# dataset

dataset : pems
# input dimension
input_dim : 963
# input sequence length
t_in : 144
# output dimension
output_dim : 7
# output sequence length
t_out : ~
